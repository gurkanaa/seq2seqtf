{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sequence to Sequence Autoencoder With Tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data read-test and training sets.\n",
    "def read_data():\n",
    "    x=sio.loadmat('vibration_nasa_1_ch5.mat')\n",
    "    x=x['dataset']\n",
    "    train_set_size=int(np.size(x,axis=1)*4/50)*10\n",
    "    train_set=x[:,:train_set_size]\n",
    "    test_set=x[:,train_set_size:]\n",
    "    normalization_constant=(np.max(np.abs(train_set)))\n",
    "    train_set/=normalization_constant\n",
    "    test_set/=normalization_constant\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "time_length=200#length of each signal frame\n",
    "batch_size=5#batch size for training\n",
    "input_dim=1#1-d time series signal\n",
    "num_units=10#number of units in LSTM cells\n",
    "#Inputs & Outputs => Data\n",
    "encoder_input=tf.placeholder(tf.float32,[time_length,batch_size,input_dim])#input placeholder\n",
    "decoder_ground_truth=tf.placeholder(tf.float32,[time_length,batch_size,input_dim])#true output placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build encoder part\n",
    "encoder_cell=tf.nn.rnn_cell.BasicLSTMCell(num_units,name='encoder',dtype=tf.float32)#LSTM cell\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "encoder_outputs,encoder_state=tf.nn.dynamic_rnn(encoder_cell,encoder_input,time_major=True,\n",
    "                                                initial_state=initial_state)#Run dynamic RNN\n",
    "#build decoder part\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units,dtype=tf.float32,name='decoder')\n",
    "zero_input=tf.constant(np.zeros([time_length,batch_size,input_dim]),dtype=tf.float32)\n",
    "decoder_outputs,decoder_state=tf.nn.dynamic_rnn(decoder_cell,zero_input,time_major=True,initial_state=encoder_state)\n",
    "##The outputs are calculated by linear combination of the states, because the problem is a regression problem\n",
    "##For this operation we need a weight kernel and a bias kernel\n",
    "W2=tf.Variable(np.random.rand(num_units,input_dim),dtype=tf.float32)\n",
    "b2=tf.Variable(np.random.rand(input_dim,1),dtype=tf.float32)\n",
    "#output vector (It is called as vector because outputs are created as vectors of size truncated bp length)\n",
    "predictions=[tf.matmul(decoder_outputs[i,:,:],W2)+b2 for i in range(decoder_outputs.get_shape().as_list()[0])]\n",
    "predictions=tf.reshape(predictions,[time_length,batch_size,input_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error=tf.losses.mean_squared_error(decoder_ground_truth,predictions)#error\n",
    "\n",
    "train_step=tf.train.RMSPropOptimizer(0.001).minimize(error)#optimizer (applies 1 step training)- \n",
    "                                                        #AdaGrad is preferred for its convergence rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())#initialize variables\n",
    "    train_set,test_set=read_data()#generate time series\n",
    "    k=np.reshape(train_set,[200,5,-1],order='F')\n",
    "    \n",
    "    data_length=np.size(k,axis=2)\n",
    "    for epoch in range(100):\n",
    "        for frame_id in range(data_length):\n",
    "            batch_in=np.expand_dims(k[:,:,frame_id],axis=2)\n",
    "            batch_out=batch_in\n",
    "            [_error,_train_step,_predictions]=sess.run(\n",
    "                    [error,train_step,predictions],\n",
    "                    feed_dict={\n",
    "                        encoder_input:batch_in,\n",
    "                        decoder_ground_truth:batch_out\n",
    "                    }\n",
    "                )\n",
    "            if frame_id%100==0:\n",
    "                print(frame_id,_error)\n",
    "                '''plt.figure(1)\n",
    "                plt.plot(batch_in[:,1,0])\n",
    "                plt.figure(2)\n",
    "                plt.plot(_predictions[:,1,0])\n",
    "                plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k.shape)\n",
    "plt.figure(1)\n",
    "plt.plot(k[:,2,1])\n",
    "plt.figure(2)\n",
    "plt.plot(train_set[:,7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(gatech)",
   "language": "python",
   "name": "gatech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
